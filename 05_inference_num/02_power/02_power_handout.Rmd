---
title: "Statistical Power"
author: "Chapter 5, Lab 2"
date: "OpenIntro Biostatistics"

fontsize: 11pt
geometry: margin=1in

output:
  pdf_document:
    includes:
      in_header: ../../header.tex
    fig_width: 5
    fig_height: 3.5
---

\begin{small}
	
	\textbf{Topics}
	\begin{itemize}
	  \item Controlling Type I error
	  \item Controlling Type II error
	  \item Power and sample size calculations
	\end{itemize}
	
\end{small}

Most studies are done to establish evidence in favor of an alternative hypothesis. The **power** of a statistical test is the probability that the test will reject the null hypothesis when the alternative hypothesis is true. Power depends on the hypothesized difference between two population means ($|\mu_1 - \mu_2|$), the population standard deviation of each group ($\sigma_1$, $\sigma_2$), and the sample size of each group ($n_1$, $n_2$). Usually, a study team can only control sample size.

The power of a test can be expressed as $P$(reject $H_0$|$H_A$ is true) = $1 - \beta$, where $\beta$ is the probability of making a Type II error (failing to reject $H_0$ when $H_A$ is true). Recall that the significance level of a test, $\alpha$, is the probability of making a Type I error (rejecting $H_0$ when $H_0$ is true).

\vspace{0.5cm}

\begin{center}
\begin{tabular}{|l|l|l|} \hline
& \multicolumn{2}{c|}{\textbf{Result of test}} \\ \hline
\textbf{State of nature}  & \textbf{Reject $H_0$}  & \textbf{Fail to reject $H_0$}\\
\hline
 & Type I error, & No error,\\
$H_0$ is true & probability = $\alpha$ & probability = $1 - \alpha$ \\
 & (false positive) & (true positive) \\ \hline
 &  No error, & Type II error,\\
$H_A$ is true &  probability = $1 - \beta$ & probability = $\beta$\\
 & (true positive) & (false negative) \\ \hline
\end{tabular}
\end{center}  

\vspace{0.5cm}

This lab uses simulation to explore how Type I and Type II error are controlled, and examines factors influencing the power of a statistical test. The last section introduces the formulas for power and sample size calculations in the two-group setting.

The material in this lab corresponds to Section 5.4 of *OpenIntro Biostatistics*.

### Introduction

Suppose a pharmaceutical company has developed a new drug for lowering blood pressure and is planning a clinical trial to test the drug's effectiveness. Participants are randomized to one of two treatments, either a currently accepted medication or the new drug. At the end of the study, a hypothesis test will be conducted to assess whether there is evidence that the new drug performs better than the standard medication. 

The following sections examine simulations performed under two scenarios: 1) the null hypothesis is true, and there is no difference in population mean blood pressure between the two groups, or 2) the alternative hypothesis is true, and there is a difference in population mean blood pressure between the two groups. For clinical trial data, it is standard practice to test the two-sided alternative. 

Note that for this setting, there is no actual population of individuals taking the new drug (since the drug is not yet available on the market). Regardless, the observations on the participants assigned to take the new drug are treated as if they are a random sample from a hypothetical population.

### Controlling Type I error

Suppose that the null hypothesis $H_0: \mu_{treatment} = \mu_{control}$ is true. Let the mean systolic blood pressure in both groups be 140 mm Hg, with standard deviation of 10 mm Hg; assume that blood pressures are normally distributed.

1. Run the following code to simulate blood pressure values for 50 individuals in the control group and 50 individuals in the treatment group, stored in the vectors \texttt{control} and \texttt{treatment}. 

    The \texttt{rnorm()} function draws \texttt{n} random numbers from a normal distribution with a given mean and standard deviation.

    ```{r initial test equal means}
#set the parameters
control.mean = 140
treatment.mean = 140
control.sigma = treatment.sigma = 10
control.n = treatment.n = 50

alpha = 0.05

#set seed
set.seed(2018)

#simulate data
control = rnorm(n = control.n, mean = control.mean, sd = control.sigma)
treatment = rnorm(n = treatment.n, mean = treatment.mean, sd = treatment.sigma)

#conduct the test

```

    a) Calculate $\overline{x}$ and $s$ for \texttt{control} and \texttt{treatment} to confirm that the simulated values seem plausible given the specified parameters for $\mu$ and $\sigma$. Would you expect $\overline{x}$ and $s$ to be exactly the same as the parameter values? Why or why not?
  
    b) Using \texttt{t.test()}, conduct a two-sided test of the null hypothesis from the simulated data. Summarize your conclusions.
  
2. Run the following code to repeat the simulation 1,000 times. With each iteration, the code draws a new set of control and treatment values, conducts the two-sample $t$-test, and records the $p$-value. The logical vector \texttt{reject} records whether the $p$-value for a particular iteration was less than or equal to $\alpha$.

    ```{r repeated tests equal means, eval = FALSE}
#set parameters
control.mean = 140
treatment.mean = 140
control.sigma = treatment.sigma = 10
control.n = treatment.n = 50

alpha = 0.05
replicates = 1000

#set seed
set.seed(2018)

#create empty list
p.values = vector("numeric", replicates)

#run simulations
for (k in 1:replicates){
  control = rnorm(n = control.n, mean = control.mean, sd = control.sigma)
  treatment = rnorm(n = treatment.n, mean = treatment.mean, sd = treatment.sigma)
  
  p.values[k] = t.test(control, treatment, alternative = "two.sided", mu = 0,
       conf.level = 1 - alpha)$p.val
}

#view results
reject = (p.values <= alpha)
table(reject)
```

    a) With 50 individuals in each group, what percentage of tests result in the (incorrect) conclusion that the two population means are different?
  
    b) Does Type I error rate change with sample size? Modify the simulation code to assess sample sizes of 100, 1,000, and 10,000.


### Controlling Type II error

Now, suppose that the alternative hypothesis $H_A: \mu_{treatment} \neq \mu_{control}$ is true. Let the mean systolic blood pressure be 140 mm Hg in the control group and 138 mm Hg in the treatment group.

3. Run the code shown in the template to simulate blood pressure values for 25 individuals in the control group and 25 individuals in the treatment group.

    a) Conduct a two-sided test of the null hypothesis from the simulated data. What is the conclusion of the test?
    
    b) Is the conclusion from part a) correct?
    
4. *Power and Sample Size*. Run the code chunk shown in the template to repeat the simulation 1,000 times. 

    a) With 25 individuals in each group, how many tests result in the incorrect conclusion that the two population means are not different?
    
    b) Estimate the power of the two-sample test when each group has $n = 25$, $\sigma= 10$, and $\mu_{treatment} - \mu_{control} = -2$ mm Hg. Recall that power refers to the probability of rejecting $H_0$ when $H_A$ is true. 
    
    c) How does power change with increasing sample size? Estimate the power of the test as $n$ changes to 50, 100, 200, and 300.
    
    d) Run the code chunk shown in the template to createa  plot of power against sample size. The code uses the command \texttt{power.t.test()} to calculate power; this command will be discussed later in the lab. Does power seem linear in relation to sample size?
    
5. *Power and Standard Deviation*. 

    ```{r initial test unequal means}
#set the parameters
control.mean = 140
treatment.mean = 138
control.sigma = treatment.sigma = 10
control.n = treatment.n = 25

alpha = 0.05

#set seed
set.seed(2018)

#simulate data
control = rnorm(n = control.n, mean = control.mean, sd = control.sigma)
treatment = rnorm(n = treatment.n, mean = treatment.mean, sd = treatment.sigma)

#conduct the test

```

```{r power and sample size}
#set parameters
control.mean = 140
treatment.mean = 138
control.sigma = treatment.sigma = 10
control.n = treatment.n = 25

alpha = 0.05
replicates = 1000

#set seed
set.seed(2018)

#create empty list
p.values = vector("numeric", replicates)

#run repeated t-tests and record p-values
for (k in 1:replicates){
  control = rnorm(n = control.n, mean = control.mean, sd = control.sigma)
  treatment = rnorm(n = treatment.n, mean = treatment.mean, sd = treatment.sigma)
  
  p.values[k] = t.test(control, treatment, alternative = "two.sided", mu = 0,
       conf.level = 1 - alpha)$p.val
}

reject = (p.values <= alpha)
table(reject)
```

    ```{r power vs sample size plot}
#set parameters
min.n = 10
max.n = 300

sample.size.range = min.n:max.n

#create empty lists
power = vector("numeric", max.n - min.n + 1)
sample.size = vector("numeric", max.n - min.n + 1)

#calculate power
for (k in sample.size.range){
  sample.size[k] = k
  power[k] = power.t.test(n = k, delta = 3, sd = treatment.sigma, type = "two.sample",
               alternative = "two.sided", strict = TRUE)$power
}

#create plot
plot(power ~ sample.size)
```  

### Power and sample size calculations
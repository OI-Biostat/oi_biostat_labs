---
title: "Understanding $R^2$"
author: "Chapter 6, Lab 3: Template"
date: "OpenIntro Biostatistics"

fontsize: 11pt
geometry: margin=1in
output: pdf_document
---

\begin{small}
	
	\textbf{Topics}
	\begin{itemize}
	  \item $R^2$ with simulated data
	  \item $R^2$ with the PREVEND data
	\end{itemize}
	
\end{small}

The correlation coefficient $r$ measures the strength of the linear relationship between two variables. However, it is more common to measure the strength of a linear fit using $r^2$, which is usually written as $R^2$ in the context of regression.

This lab first uses simulated data to explore the idea behind the quantity $R^2$, then provides an example of using $R^2$ to assess the strength of the linear fit of a regression model.

The material in this lab corresponds to Section 6.3.2 of *OpenIntro Biostatistics*.

\vspace{0.5cm}


### Introduction

The quantity $R^2$ describes the amount of variation in the response variable that is explained by the least squares line:

\[R^2 = \dfrac{\text{variance of predicted $y$-values}}{\text{variance of observed $y$-values}} = \dfrac{\text{Var}(\hat{y}_i)}{\text{Var}(y_i)} \]

$R^2$ can also be calculated using the following formula:

\[R^2 = \dfrac{\text{variance of observed $y$-values} - \text{variance of residuals} }{\text{variance of observed $y$-values}} = \dfrac{\text{Var}(y_i) - \text{Var}(e_i)}{\text{Var}(y_i)}\]


\newpage

### $R^2$ with simulated data

A simulation can be conducted in which $y$-values are sampled according to a population regression model $y = \beta_0 + \beta_1x + \epsilon$, where the parameters $\beta_0$, $\beta_1$, and the standard deviation of $\epsilon$ are known. Recall that $\epsilon$ is a normally distributed error term with mean 0 and standard deviation $\sigma$.

1. Run the following code chunk to simulate 100 $(x, y)$ values, where the values for $x$ are 100 numbers randomly sampled from a standard normal distribution and the values for $y$ are determined by the population model $y_i = 100 + 25x_i + \epsilon_i$, where $\epsilon_i \sim N(0, 5)$.

```{r}
#set the seed
set.seed(2017)

#simulate values
x = rnorm(100)
error = rnorm(100, 0, 5)
y = 100 + 25*x + error

#create a scatterplot with the line of best fit


#print coefficients of the regression line

```


a) Create a scatterplot of \texttt{y} versus \texttt{x} and add the line of best fit to the plot. 
    
    i. Does the line appear to be a good fit to the data?
    
    
    
    
    ii. Why do the data points not fall exactly on a line, even though the data are simulated according to a known linear relationship between $x$ and $y$?
        
    
    
    
    
    iii. How well does the regression line estimate the population parameters $\beta_0$ and $\beta_1$?
    
        
    

    
b) From a visual inspection, does it seem that the $R^2$ for this linear fit is relatively high or relatively low?
    





c) Run the code chunk shown in the template to create two histograms, one of the predicted $y$-values and one of the observed (i.e., simulated) $y$-values. Visually compare the variances of the two sets of values; do the predicted and observed $y$-values seem to have similar spread?

    
```{r, eval = FALSE}
#histogram of predicted y-values
predicted = predict(lm(y ~ x))
hist(predicted, xlim = c(20, 190))    #bounds x-axis between 20 and 190

#histogram of observed y-values
hist(y, xlim = c(20, 190))
```






d) Run the code chunk shown in the template to calculate $R^2$ from the following formula. What is the $R^2$ for this model?
    
    \[R^2 = \dfrac{\text{variance of predicted $y$-values}}{\text{variance of observed $y$-values}} = \dfrac{\text{Var}(\hat{y}_i)}{\text{Var}(y_i)} \]

```{r}
#store predicted y-values
predicted = predict(lm(y ~ x))

#observed y-values are the simulated y values
observed = y

#calculate R-squared
R.squared = var(predicted)/var(observed)
R.squared
```




    
e) Calculate the $R^2$ for the model using the following formula. Confirm that the value is the same as from using the formula in part d).
    
    \[R^2 = \dfrac{\text{variance of observed $y$-values} - \text{variance of residuals} }{\text{variance of observed $y$-values}} = \dfrac{\text{Var}(y_i) - \text{Var}(e_i)}{\text{Var}(y_i)}\]
    

```{r, eval = FALSE}
#store residual values
residuals = resid(lm(y ~ x))

#calculate R-squared
R.squared = 
```




f) To have \textsf{R} print the $R^2$ of a linear model, use the \texttt{summary(lm( ))} function as shown in the template. Confirm that this value matches the ones from the previous calculations.


```{r}
summary(lm(y ~ x))$r.squared
```






\newpage
    
2. Simulate 100 new $(x, y)$ values. Like before, the $x$ values are 100 numbers randomly sampled from a standard normal distribution and the $y$ values are determined by the population model $y_i = 100 + 25x_i + \epsilon_i$. For these data, however, the error term is distributed $N(0, 50)$.

```{r}
#clear the workspace
rm(list = ls())

#set the seed
set.seed(2017)

#simulate values

#create a scatterplot with the line of best fit

#print coefficients of the regression line

```




a) Create a scatterplot of \texttt{y} versus \texttt{x} and add the line of best fit to the plot. Does the line appear to be a good fit to the data? How well does the regression line estimate the population parameters $\beta_0$ and $\beta_1$?
    





b) Run the code chunk shown in the template to create two histograms, one of the predicted $y$-values and one of the observed (i.e., simulated) $y$-values. Visually compare the variances of the two sets of values; do the predicted and observed $y$-values seem to have similar spread?
    

```{r, eval = FALSE}
#histogram of predicted y-values
predicted = predict(lm(y ~ x))
hist(predicted, xlim = c(0, 250))    #bounds x-axis between 0 and 250

#histogram of observed y-values
hist(y, xlim = c(0, 250))
```




c) Based on the answers to parts a) and b), does it seem that the $R^2$ for this linear model is relatively high or relatively low? 
    





d) Use any method to calculate $R^2$ for the linear model.

    
```{r}
#calculate R-squared

```




3. Run the code chunk shown in the template to simulate 100 new $(x, y)$ values. 

```{r}
#clear the workspace
rm(list = ls())

#set the seed
set.seed(2017)

#simulate values
x = rnorm(100)
error = rnorm(100, 0, 5)
y = 100 + 25*x + 5*x^2 + error

#calculate R-squared

```





a) Fit a linear model predicting $y$ from $x$ to the data and calculate the $R^2$ for the model. Based on $R^2$, does the model seem to fit the data well?






b) Plot the data and add the line of best fit. Evaluate whether the linear model is a good fit to the data; how does viewing the data change the conclusion from part a)?
    
    
    
```{r}
#create a scatterplot with the line of best fit

```
    
    

### $R^2$ with the PREVEND data

4. Run the code chunk shown in the template to create \texttt{prevend.sample}, the random sample of 500 individuals from the PREVEND data used in the previous labs in this chapter.

```{r}
#load the data
library(oibiostat)
data("prevend")

#take a random sample
set.seed(5011)
sample.size = 500
prevend.sample = prevend[sample(1:nrow(prevend), sample.size, replace = FALSE), ]

#plot RFFT scores versus age

```


a) Plot RFFT scores versus age and confirm that these data seem reasonably linear.
    




b) What proportion of the variability in the observed RFFT scores is explained by the linear model predicting average RFFT score from age?

    
```{r}

```





c) Evaluate the strength of the linear relationship between RFFT score and age. Does it seem like there might be other factors that explain the variability in RFFT score?





